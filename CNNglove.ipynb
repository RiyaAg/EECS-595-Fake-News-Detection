{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyBXVN6x0dxr",
        "outputId": "28013afc-d5d8-4f99-e3c0-ce172e5867d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-7705946c4671>:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embedded_text = torch.stack([torch.tensor(self.glove_embeddings[t]) for t in tokens])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Train Loss: 0.3234739899635315, Train Accuracy: 63.10%, Validation Accuracy: 67.29%\n",
            "Epoch 2/4, Train Loss: 0.41495776176452637, Train Accuracy: 67.55%, Validation Accuracy: 66.51%\n",
            "Epoch 3/4, Train Loss: 1.1246583461761475, Train Accuracy: 78.45%, Validation Accuracy: 63.63%\n",
            "Epoch 4/4, Train Loss: 0.11450965702533722, Train Accuracy: 89.38%, Validation Accuracy: 62.85%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "from gensim.models import KeyedVectors\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "embedding_dim = 300\n",
        "glove = GloVe(name='6B', dim=embedding_dim)\n",
        "vectors = glove.vectors\n",
        "vocab = glove.stoi\n",
        "\n",
        "\n",
        "liar_dataset = load_dataset('liar')\n",
        "train_data, val_data, test_data = liar_dataset['train'], liar_dataset['validation'], liar_dataset['test']\n",
        "\n",
        "vocab_size = 30000\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "tokenizer.post_processor = processors.ByteLevel()\n",
        "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
        "tokenizer.train_from_iterator(train_data['statement'], trainer=trainer)\n",
        "\n",
        "texts = train_data['statement']\n",
        "labels = train_data['label']\n",
        "val_texts = val_data['statement']\n",
        "val_labels = val_data['label']\n",
        "label_mapping = {0: 0, 1: 0, 2: 1, 3: 1, 4: 0, 5:0}\n",
        "labels = [label_mapping[label] for label in labels]\n",
        "val_labels = [label_mapping[label] for label in val_labels]\n",
        "\n",
        "class LiarDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, glove_embeddings):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.glove_embeddings = glove_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        tokens = self.tokenizer.encode(text).ids\n",
        "        embedded_text = torch.stack([torch.tensor(self.glove_embeddings[t]) for t in tokens])\n",
        "\n",
        "        return embedded_text, label\n",
        "\n",
        "dataset = LiarDataset(texts, labels, tokenizer, vectors)\n",
        "val_dataset = LiarDataset(val_texts, val_labels, tokenizer, vectors)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Sort batch in descending order by sequence length and pad sequences\n",
        "    batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences, labels = zip(*batch)\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "    return padded_sequences, torch.tensor(labels)\n",
        "\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "class CNNForSentenceClassification(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_classes, kernel_sizes=(3, 4, 5), num_filters=100):\n",
        "        super(CNNForSentenceClassification, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vectors), freeze=True)\n",
        "        self.convolution_layers = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
        "            for kernel_size in kernel_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        convolution_outputs = [torch.relu(conv(x.permute(0, 2, 1))) for conv in self.convolution_layers]\n",
        "        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in convolution_outputs]\n",
        "\n",
        "        # Concatenate the pooled outputs\n",
        "        x = torch.cat(pooled_outputs, dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "num_classes = 2  # Number of classes in the Liar dataset\n",
        "model = CNNForSentenceClassification(embedding_dim, num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_correct_train = 0\n",
        "    total_samples_train = 0\n",
        "\n",
        "    for batch_texts, batch_labels in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_texts)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs, 1)\n",
        "        total_correct_train += (predicted_train == batch_labels).sum().item()\n",
        "        total_samples_train += batch_labels.size(0)\n",
        "\n",
        "    accuracy_train = total_correct_train / total_samples_train\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_correct_val = 0\n",
        "    total_samples_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_texts_val, batch_labels_val in val_dataloader:\n",
        "            outputs_val = model(batch_texts_val)\n",
        "            _, predicted_val = torch.max(outputs_val, 1)\n",
        "            total_correct_val += (predicted_val == batch_labels_val).sum().item()\n",
        "            total_samples_val += batch_labels_val.size(0)\n",
        "\n",
        "    accuracy_val = total_correct_val / total_samples_val\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "          f\"Train Loss: {loss.item()}, Train Accuracy: {accuracy_train * 100:.2f}%, \"\n",
        "          f\"Validation Accuracy: {accuracy_val * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "test_text = test_data[\"statement\"]\n",
        "test_labels = test_data[\"label\"]\n",
        "test_labels = [label_mapping[label] for label in test_labels]\n",
        "t_dataset = LiarDataset(test_text, test_labels, tokenizer, vectors)\n",
        "test_dataloader = DataLoader(t_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_texts, batch_labels in test_dataloader:\n",
        "        outputs = model(batch_texts)\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == batch_labels).sum().item()\n",
        "        total_samples += batch_labels.size(0)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvLJl_1NZrzn",
        "outputId": "1bf5f0ee-f12f-46a4-efb7-1c6b1e9a6f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-7705946c4671>:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  embedded_text = torch.stack([torch.tensor(self.glove_embeddings[t]) for t in tokens])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 57.99%\n"
          ]
        }
      ]
    }
  ]
}